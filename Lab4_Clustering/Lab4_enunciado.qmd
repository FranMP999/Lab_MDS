---
jupyter: lab_env
---

<h1><center>Laboratorio 4: La solicitud de Sergio ü§ó</center></h1>

<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>

### Cuerpo Docente:

- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco
- Auxiliares: Melanie Pe√±a, Valentina Rojas
- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga

### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados

- Nombre de alumno 1: Francisco Maldonado
- Nombre de alumno 2:

### **Link de repositorio de GitHub:** [Repositorio](https://github.com/FranMP999/)

## Temas a tratar
- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.
- Aplicar Pipelines y Column Transformers.
- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.

## Reglas:

- **Grupos de 2 personas**
- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.
- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.
- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.
- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.
- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.
- Pueden usar cualquier material del curso que estimen conveniente.

### Objetivos principales del laboratorio
- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.
- Familiarizarse con plotly.

El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka "for", "while"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*).

## Descripci√≥n del laboratorio

<center>
<img src="https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif" width=400 />

En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Sergio, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Sergio, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.

Sergio encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.

A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva.

## Importamos librerias utiles üò∏

```{python}
#| cell_id: 95a5533cfd6d49cfb9afc111c44d224f
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 15
#| execution_start: 1714107106552
#| source_hash: null
import numpy as np
import pandas as pd
import time

from sklearn import datasets

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots


from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

from sklearn.ensemble import IsolationForest
from sklearn.mixture import GaussianMixture

from sklearn.metrics import silhouette_score

# Random State permite controlar la aleatoridad.
# Es decir, permite generar los mismos n√∫meros aleatorios en distintas ejecuciones.
RANDOM_STATE = 99

#Imports de algoritmos de clusters


#Paleta de colores auxiliar
import matplotlib.pyplot as plt
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
```

## 1. Estudio de Performance üìà [10 Puntos]


<center>
<img src="https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif" width=300>

Don Sergio les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.

Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.

Uno de los requisitos establecidos por Sergio es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:

<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />

En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida.

Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:
1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]
2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]
3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]


> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.

> ‚ùó Recuerde que es obligatorio el uso de plotly.

```{python}
#| cell_id: 7f7c25e366754595b13fc2e8116f65a0
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 78
#| execution_start: 1714107108441
#| source_hash: null
"""
En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.
‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó
"""

# Datos a utilizar

# Configuracion
n_samples = 5000 #Este par√°metro si lo pueden modificar

def create_data(n_samples):

    # Lunas
    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)
    # Blobs
    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)
    # Datos desiguales
    transformation = [[0.6, -0.6], [-0.4, 0.8]]
    mutated = (np.dot(blobs[0], transformation), blobs[1])

    # Generamos Dataset
    dataset = {
        'moons':{
            'x': moons[0], 'classes': moons[1], 'n_cluster': 2
        },
        'blobs':{
            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3
        },
        'mutated':{
            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3
        }
    }
    return dataset

data_sets = create_data(n_samples)
```

**Respuestas:**

```{python}
pd_data_sets = {
    key : pd.DataFrame({'x1': data_sets[key]['x'][:,0],
                        'x2': data_sets[key]['x'][:,1],
   #                     'class':  data_sets[key]['classes'],
                       })
    for key in data_sets.keys()
               }
metodos = ["KMeans", "GMM", "WARD", "DBSCAN"]
resultados = {key: dict() for key in [(x, y) for x in metodos for y in data_sets.keys()]}
```

```{python}
for key in data_sets.keys():
    times = [time.time()]

    resultados[("KMeans", key)]["labels"] = (
        KMeans(
            n_clusters=2,
            random_state=RANDOM_STATE,
            n_init=10,
         ).fit(pd_data_sets[key])
        ).labels_
    times += [time.time()]

    resultados[("GMM", key)]["labels"] = (
        GaussianMixture(n_components = 2, random_state=RANDOM_STATE)
        .fit_predict(pd_data_sets[key])
    )
    times += [time.time()]

    resultados[("WARD", key)]["labels"] = (
        AgglomerativeClustering(n_clusters=2, linkage="ward")
        .fit_predict(pd_data_sets[key])
    )
    times += [time.time()]

    resultados[("DBSCAN", key)]["labels"] = (
         DBSCAN(eps=0.1, min_samples=2)
        .fit_predict(pd_data_sets[key])
    )
    times += [time.time()]

    for i, metodo in enumerate(["KMeans", "GMM", "WARD", "DBSCAN"]):
        resultados[(metodo, key)]["score"] = silhouette_score(
                    pd_data_sets[key],
                    resultados[(metodo, key)]["labels"],
                )
        resultados[(metodo, key)]["time"] = times[i+1] - times[i]
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 737}
fig = make_subplots(
    rows=3, cols=4,
    subplot_titles=metodos)

for i, key in enumerate(data_sets.keys()):
    for j, method in enumerate(metodos):
        fig.add_trace(
            go.Scatter(
                x=pd_data_sets[key]['x1'], y=pd_data_sets[key]['x2'],
                mode='markers', marker=dict(color=resultados[(method, key)]["labels"], colorscale='Viridis'),
            ),
            row=i+1, col=j+1
            )
        anotacion = (
                f"{round(resultados[(method, key)] ['time'], 2)} [s] "
                + f"s: {round(resultados[(method,key)]['score'], 2)}"
            )

        fig.add_annotation(
            y=-1,  # y = 1 is the top of the plot area; the top is typically uncluttered, so placing
                      # the bottom of the title slightly above the graph region works on a wide variety of graphs
            text=anotacion,
            showarrow=False,
            row=i+1,col=j+1
            )


fig.update_layout(height=700, width=900, title_text="Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica", showlegend=False,)


fig.show()
#px.scatter(pd_data_sets['moons'], x='x1', y='x2', color='class')
```

## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]

<center>
<img src="https://i.gifer.com/2Hci.gif" width=400 />

Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Sergio* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:

- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)
- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)
- *Age*: Edad actual de los pasajeros
- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)
- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)
- *Flight distance*: Distancia del vuelo de este viaje
- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)
- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada
- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea
- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta
- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida
- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea
- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento
- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo
- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo
- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas
- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje
- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in
- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo
- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza
- *Departure Delay in Minutes*: Minutos de retraso en la salida
- *Arrival Delay in Minutes*: Minutos de retraso en la llegada

En consideraci√≥n de lo anterior, realice las siguientes tareas:

0. Ingeste el dataset a su ambiente de trabajo.

1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]

2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]

3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]

4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]

5. De acuerdo con los resultados obtenidos en 5, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]

**Respuesta:**

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 1000}
# Carga de datos
df_aero = pd.read_parquet("aerolineas_lucer.parquet")
numerical_columns = df_aero.select_dtypes(include=np.number).columns.drop('id') #no se incluye la columna id, pues distintos viajes del mismo cliente se pueden considerar como datos independientes respecto a satisfacci√≥n
for column in numerical_columns:
    fig = px.histogram(df_aero, x=column)
    fig.show()
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 717}
selected_columns = ["Cleanliness", "Ease of Online booking", "Seat comfort", "Inflight service"]
px.imshow(df_aero[numerical_columns].corr(), height=700)
```

**Respuestas:**

1. En general no se deben usar variables categ√≥ricas para alimentar un algoritmo no supervisado, pues su funcionamiento se sustenta en la aplicaci√≥n de medidas de distancias que no son aplicables sobre variables categ√≥ricas, por lo que incluirlas s√≥lo corresponde a un sesgo extra sobre el entrenamiento, sobre informaci√≥n cuantitativa que no est√° realmente presente en los datos.

2. De la distribuci√≥n de las variables se puede apreciar que las variables menos informativas parecen ser *Arrival Delay in minutes* y *Departure Delay in minutes, ya que est√°n demasiado concentradas, son casi una delta de dirac, con una cola muy delgada, poco informativa, podemos concluir que no nos permitir√° distinguir los datos notablemente. **Age** y **Flight Distance** son variables continuas que pueden ser de utilidad, sin embargo al ser de naturaleza distinta al resto de variables podr√≠an dificultar a los algoritmos no supervisados la tarea de encontrar patrones en el comportamiento conjunto de las variables. El resto de variables son de evaluaci√≥n del 0 al 5.

3. Dado que se seleccionan s√≥lo variables de evaluaci√≥n de 0 a 5 no hace falta escalar los datos.

5. En base a la matriz de correlaciones se puede observar la correlaci√≥n de Cleanliness con diversas otras variables, por lo que se decide dejar esta entre las columnas seleccionadas. M√°s all√° de esto se distinguen 4 "cuadrados" de alta correlaci√≥n por lo que se decide seleccionar un atributo de cada cuadrado, intentando escoger aquella que presente correlaciones m√°s altas dentro del "cuadrado". Buscando con esta decisi√≥n escoger aquellas variables que encapsulen m√°s informaci√≥n de las otras variables del cuadrado que no se seleccionar√°n.  


## 3. Preprocesamiento üé≠. [10 puntos]

Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.

Bas√°ndose en los an√°lisis realizados anteriormente:
1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]
2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]

**Respuestas:**

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 542}
# Escriba su c√≥digo aqu√≠

# Se usaran las 4 columnas previamente seleccionadas para proyectar a 2 dimensiones
reductor = [('reduce_dim', PCA(n_components=2))]
pipe = Pipeline(reductor)
array_reducted = pipe.fit_transform(df_aero[selected_columns])
df_reducted = pd.DataFrame({
    'x': array_reducted[:,0],
    'y': array_reducted[:,1],
                       })
# Gr√°fico
fig = px.scatter(
        df_reducted,
        x="x",
        y="y",
        range_x=(df_reducted["x"].min() - 1, df_reducted["x"].max() + 1),
        range_y=(df_reducted["y"].min() - 1, df_reducted["y"].max() + 1),
    )
fig.show()
```

En la proyecci√≥n se identifican claras agrupaciones de puntos, podemos reconocer 9 columnas distinguidas, formadas cada una por 6 grupos distinguibles, cada uno  conformado por 5 puntos muy densos de datos.

## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]

<center>
<img src="https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif" width=250>

Con el objetivo de mantener la claridad en su an√°lisis, Don Sergio le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.

1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]

2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]

3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]

**Respuestas:**

```{python}
#| cell_id: be86896911244aa89e3b5f3f00a286af
#| deepnote_cell_type: code
# Escriba su c√≥digo aqu√≠

outliers = [('outliers', IsolationForest(
    random_state=RANDOM_STATE,
    contamination=0.01,
))]
pipe = Pipeline(outliers)
df_reducted["outlier_labels"] = (
        pipe
        .fit(df_aero[selected_columns])
        .predict(df_aero[selected_columns])
        )
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 542}
fig = px.scatter(
    df_reducted,
    x="x",
    y="y",
    range_x=(df_reducted["x"].min() - 1, df_reducted["x"].max() + 1),
    range_y=(df_reducted["y"].min() - 1, df_reducted["y"].max() + 1),
    color="outlier_labels"
    )
fig.show()
```

Bas√°ndonos exclusivamente en la proyecci√≥n a dos dimensiones podemos concluir que la detecci√≥n de anomal√≠as es deficiente, pudiendo identificar una anomal√≠a clara en el an√°lisis gr√°fico, pero identificando un gran n√∫mero de datos que pertenecen a la distribuci√≥n principal como anomal√≠as. Sin embargo es dificil adjudicar esto a un problema del algoritmo, pues de lo que identificamos en la proyecci√≥n, s√≥lo existe un outlier, y sin embargo le exigimos la identificaci√≥n de un 10% de los datos, en el orden de cientos.

## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]

Motivado por incrementar su fortuna, Don Sergio le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:

1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]
2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]

> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea.

**Respuestas:**

```{python}
#| cell_id: 6d3d1bb3fda14321984466d9101a775a
#| deepnote_cell_type: code
#| colab: {base_uri: 'https://localhost:8080/', height: 1000}
# Escriba su c√≥digo aqu√≠

gmms = [
    GaussianMixture(n_components=n, random_state=RANDOM_STATE).fit(df_aero[selected_columns])
    for n in range(3, 9)
]

clusters = [
    gmm.predict(df_aero[selected_columns])
    for gmm in gmms
]


gmm_labels = pd.DataFrame(np.array(clusters)).T
gmm_labels.columns = range(3, 9)

gmm_labels["x"] = df_reducted.x
gmm_labels["y"] = df_reducted.y

gmm_labels = gmm_labels.melt(
    id_vars=["x", "y"], var_name="n_components", value_name="label"
)
gmm_labels["label"] = gmm_labels["label"].astype(str)

fig = px.scatter(
    gmm_labels,
    x="x",
    y="y",
    facet_row="n_components",
    color="label",
    height=1600,
)
fig.show()
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 1000}
scores_df = pd.DataFrame({
    "n_components" : range(3,9),
    "aic" : [
        gmm.aic(df_aero[selected_columns])
        for gmm in gmms
    ],
    "bic" : [
        gmm.bic(df_aero[selected_columns])
        for gmm in gmms
    ]}).set_index("n_components")

fig = px.line(scores_df)
fig.show()
fig = px.line(scores_df.loc[:7,:])
fig.show()
```

**Respuesta:** Ambos son criterios de selecci√≥n de modelos basados en verosimilitud, balanceando capacidad de fiteo y complejidad,
en general BIC tiende a penalizar m√°s fuertemente modelos con muchos par√°metros que AIC, buscando  escoger modelos m√°s simples. Pero
ambos consisten en maximizar la verosimilitud penalizando el n√∫mero de par√°metros y est√°n basados en la teor√≠a de la informaci√≥n.
Se distingue con claridad del an√°lisis de ambos estad√≠sticos que el n√∫mero de clusters m√°s apto entre los seleccionados es $8$.



## 6. An√°lisis de resultados üìä [10 puntos]

Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:

1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]

2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]

3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]

4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]

5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]

**Respuestas:**

```{python}
#| cell_id: 9abf4dbc643e40cebe99fcb1ff3ff413
#| deepnote_cell_type: code
#| scrolled: true
#| colab: {base_uri: 'https://localhost:8080/', height: 542}
# Escriba su c√≥digo aqu√≠
df_reducted["gmm_labels"] = clusters[-1]
fig = px.scatter(
    df_reducted,
    x="x",
    y="y",
    range_x=(df_reducted["x"].min() - 1, df_reducted["x"].max() + 1),
    range_y=(df_reducted["y"].min() - 1, df_reducted["y"].max() + 1),
    color="gmm_labels",
    title="Proyecci√≥n de clusters en 2D"
    )
fig.show()
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 264}
import itertools
from scipy import linalg
import matplotlib as mpl

color_iter = itertools.cycle(["navy", "c", "cornflowerblue", "gold", "darkorange"])

means = np.array([
    df_reducted[df_reducted["gmm_labels"] == i].mean()[["x", "y"]]
    for i in range(0,8)
])

covariances = np.array([
    df_reducted[df_reducted["gmm_labels"] == i][["x", "y"]].cov()
    for i in range(0,8)
])

splot = plt.subplot(2, 1, 1)
for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):
    v, w = linalg.eigh(covar)
    v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
    u = w[0] / linalg.norm(w[0])
    # as the DP will not use every component it has access to
    # unless it needs it, we shouldn't plot the redundant
    # components.
    if not np.any(clusters[-1] == i):
        continue
    plt.scatter(df_reducted["x"], df_reducted["y"], 0.8, color=color)

    # Plot an ellipse to show the Gaussian component
    angle = np.arctan(u[1] / u[0])
    angle = 180.0 * angle / np.pi  # convert to degrees
    ell = mpl.patches.Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.5)
    splot.add_artist(ell)

plt.xlim(df_reducted["x"].min() - 1, df_reducted["x"].max() + 1)
plt.ylim(df_reducted["y"].min() - 1, df_reducted["y"].max() + 1)
plt.xticks(())
plt.yticks(())
plt.title(r"Elipses de confianza ($1\sigma$) para los distintos clusters")
plt.figure(figsize=(20,15))

```

Dada la poca interpretabilidad que tienen los promedios y covarianzas que caracterizan la clusterizaci√≥n Gaussiana en cuatro dimensiones, se proyectan a dos dimensiones para graficar y poder interpretar. Lamentablemente no logr√© graficar las elipses en plotly, sin embargo estas permiten ver que los clusters proyectados no logran separar correctamente las agrupaciones de datos, y pareciera que el algoritmo no logra hacer un trabajo adecuado para reconocer patrones en este conjunto de datos.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 542}
reductor2 = [('reduce_dim', PCA(n_components=3))]
pipe_reductor2 = Pipeline(reductor2)
array_reducted2 = pipe_reductor2.fit_transform(df_aero[selected_columns])
df_reducted2 = pd.DataFrame({
    'x': array_reducted2[:,0],
    'y': array_reducted2[:,1],
    'z': array_reducted2[:,2],
    "gmm_labels" : clusters[-1],
                       })
# Gr√°fico
fig = px.scatter_3d(
    df_reducted2,
    x="x",
    y="y",
    z="z",
    range_x=(df_reducted2["x"].min() - 1, df_reducted2["x"].max() + 1),
    range_y=(df_reducted2["y"].min() - 1, df_reducted2["y"].max() + 1),
    range_z=(df_reducted2["z"].min() - 1, df_reducted2["z"].max() + 1),
    color="gmm_labels"
    )
fig.show()
```

En tres dimensiones se logra apreciar una diferenciaci√≥n mucho m√°s clara entre los clusters 4,5,6, sin embargo sigue habiendo un fuerte solape entre los clusters 0,1,2,3 y 7, solape que posiblemente se solucione en cuatro dimensiones. La visualizaci√≥n en tres dimensiones definitivamente entrega un insight m√°s profundo del desempe√±o del algoritmo, sin embargo la naturaleza de la data parece no ser la m√°s apropiada para este algoritmo de clasificaci√≥n, por no presentar conjuntos de forma "elipsoide" que la mezcla gaussiana presupone.

